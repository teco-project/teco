//===----------------------------------------------------------------------===//
//
// This source file is part of the Teco open source project
//
// Copyright (c) 2022-2023 the Teco project authors
// Licensed under Apache License v2.0
//
// See LICENSE.txt for license information
//
// SPDX-License-Identifier: Apache-2.0
//
//===----------------------------------------------------------------------===//

// THIS FILE IS AUTOMATICALLY GENERATED by TecoServiceGenerator.
// DO NOT EDIT.

import Logging
import NIOCore
import TecoCore

extension Dlc {
    /// ModifySparkApp请求参数结构体
    public struct ModifySparkAppRequest: TCRequest {
        /// spark作业名
        public let appName: String

        /// spark作业类型，1代表spark jar作业，2代表spark streaming作业
        public let appType: Int64

        /// 执行spark作业的数据引擎名称
        public let dataEngine: String

        /// spark作业程序包文件路径
        public let appFile: String

        /// 数据访问策略，CAM Role arn
        public let roleArn: Int64

        /// 指定的Driver规格，当前支持：small（默认，1cu）、medium（2cu）、large（4cu）、xlarge（8cu）
        public let appDriverSize: String

        /// 指定的Executor规格，当前支持：small（默认，1cu）、medium（2cu）、large（4cu）、xlarge（8cu）
        public let appExecutorSize: String

        /// spark作业executor个数
        public let appExecutorNums: Int64

        /// spark作业Id
        public let sparkAppId: String

        /// 该字段已下线，请使用字段Datasource
        public let eni: String?

        /// spark作业程序包是否本地上传，cos：存放与cos，lakefs：本地上传（控制台使用，该方式不支持直接接口调用）
        public let isLocal: String?

        /// spark作业主类
        public let mainClass: String?

        /// spark配置，以换行符分隔
        public let appConf: String?

        /// spark 作业依赖jar包是否本地上传，cos：存放与cos，lakefs：本地上传（控制台使用，该方式不支持直接接口调用）
        public let isLocalJars: String?

        /// spark 作业依赖jar包（--jars），以逗号分隔
        public let appJars: String?

        /// spark作业依赖文件资源是否本地上传，cos：存放与cos，lakefs：本地上传（控制台使用，该方式不支持直接接口调用）
        public let isLocalFiles: String?

        /// spark作业依赖文件资源（--files）（非jar、zip），以逗号分隔
        public let appFiles: String?

        /// pyspark：依赖上传方式，cos：存放与cos，lakefs：本地上传（控制台使用，该方式不支持直接接口调用）
        public let isLocalPythonFiles: String?

        /// pyspark作业依赖python资源（--py-files），支持py/zip/egg等归档格式，多文件以逗号分隔
        public let appPythonFiles: String?

        /// spark作业程序入参
        public let cmdArgs: String?

        /// 最大重试次数，只对spark流任务生效
        public let maxRetries: Int64?

        /// 数据源名
        public let dataSource: String?

        /// spark作业依赖archives资源是否本地上传，cos：存放与cos，lakefs：本地上传（控制台使用，该方式不支持直接接口调用）
        public let isLocalArchives: String?

        /// spark作业依赖archives资源（--archives），支持tar.gz/tgz/tar等归档格式，以逗号分隔
        public let appArchives: String?

        /// Spark Image 版本号
        public let sparkImage: String?

        /// Spark Image 版本名称
        public let sparkImageVersion: String?

        /// 指定的Executor数量（最大值），默认为1，当开启动态分配有效，若未开启，则该值等于AppExecutorNums
        public let appExecutorMaxNumbers: Int64?

        /// 关联dlc查询脚本
        public let sessionId: String?

        /// 任务资源配置是否继承集群配置模板：0（默认）不继承、1：继承
        public let isInherit: UInt64?

        /// 是否使用session脚本的sql运行任务：false：否，true：是
        public let isSessionStarted: Bool?

        public init(appName: String, appType: Int64, dataEngine: String, appFile: String, roleArn: Int64, appDriverSize: String, appExecutorSize: String, appExecutorNums: Int64, sparkAppId: String, eni: String? = nil, isLocal: String? = nil, mainClass: String? = nil, appConf: String? = nil, isLocalJars: String? = nil, appJars: String? = nil, isLocalFiles: String? = nil, appFiles: String? = nil, isLocalPythonFiles: String? = nil, appPythonFiles: String? = nil, cmdArgs: String? = nil, maxRetries: Int64? = nil, dataSource: String? = nil, isLocalArchives: String? = nil, appArchives: String? = nil, sparkImage: String? = nil, sparkImageVersion: String? = nil, appExecutorMaxNumbers: Int64? = nil, sessionId: String? = nil, isInherit: UInt64? = nil, isSessionStarted: Bool? = nil) {
            self.appName = appName
            self.appType = appType
            self.dataEngine = dataEngine
            self.appFile = appFile
            self.roleArn = roleArn
            self.appDriverSize = appDriverSize
            self.appExecutorSize = appExecutorSize
            self.appExecutorNums = appExecutorNums
            self.sparkAppId = sparkAppId
            self.eni = eni
            self.isLocal = isLocal
            self.mainClass = mainClass
            self.appConf = appConf
            self.isLocalJars = isLocalJars
            self.appJars = appJars
            self.isLocalFiles = isLocalFiles
            self.appFiles = appFiles
            self.isLocalPythonFiles = isLocalPythonFiles
            self.appPythonFiles = appPythonFiles
            self.cmdArgs = cmdArgs
            self.maxRetries = maxRetries
            self.dataSource = dataSource
            self.isLocalArchives = isLocalArchives
            self.appArchives = appArchives
            self.sparkImage = sparkImage
            self.sparkImageVersion = sparkImageVersion
            self.appExecutorMaxNumbers = appExecutorMaxNumbers
            self.sessionId = sessionId
            self.isInherit = isInherit
            self.isSessionStarted = isSessionStarted
        }

        enum CodingKeys: String, CodingKey {
            case appName = "AppName"
            case appType = "AppType"
            case dataEngine = "DataEngine"
            case appFile = "AppFile"
            case roleArn = "RoleArn"
            case appDriverSize = "AppDriverSize"
            case appExecutorSize = "AppExecutorSize"
            case appExecutorNums = "AppExecutorNums"
            case sparkAppId = "SparkAppId"
            case eni = "Eni"
            case isLocal = "IsLocal"
            case mainClass = "MainClass"
            case appConf = "AppConf"
            case isLocalJars = "IsLocalJars"
            case appJars = "AppJars"
            case isLocalFiles = "IsLocalFiles"
            case appFiles = "AppFiles"
            case isLocalPythonFiles = "IsLocalPythonFiles"
            case appPythonFiles = "AppPythonFiles"
            case cmdArgs = "CmdArgs"
            case maxRetries = "MaxRetries"
            case dataSource = "DataSource"
            case isLocalArchives = "IsLocalArchives"
            case appArchives = "AppArchives"
            case sparkImage = "SparkImage"
            case sparkImageVersion = "SparkImageVersion"
            case appExecutorMaxNumbers = "AppExecutorMaxNumbers"
            case sessionId = "SessionId"
            case isInherit = "IsInherit"
            case isSessionStarted = "IsSessionStarted"
        }
    }

    /// ModifySparkApp返回参数结构体
    public struct ModifySparkAppResponse: TCResponse {
        /// 唯一请求 ID，每次请求都会返回。定位问题时需要提供该次请求的 RequestId。
        public let requestId: String

        enum CodingKeys: String, CodingKey {
            case requestId = "RequestId"
        }
    }

    /// 更新spark作业
    @inlinable @discardableResult
    public func modifySparkApp(_ input: ModifySparkAppRequest, region: TCRegion? = nil, logger: Logger = TCClient.loggingDisabled, on eventLoop: EventLoop? = nil) -> EventLoopFuture<ModifySparkAppResponse> {
        self.client.execute(action: "ModifySparkApp", region: region, serviceConfig: self.config, input: input, logger: logger, on: eventLoop)
    }

    /// 更新spark作业
    @inlinable @discardableResult
    public func modifySparkApp(_ input: ModifySparkAppRequest, region: TCRegion? = nil, logger: Logger = TCClient.loggingDisabled, on eventLoop: EventLoop? = nil) async throws -> ModifySparkAppResponse {
        try await self.client.execute(action: "ModifySparkApp", region: region, serviceConfig: self.config, input: input, logger: logger, on: eventLoop).get()
    }

    /// 更新spark作业
    @inlinable @discardableResult
    public func modifySparkApp(appName: String, appType: Int64, dataEngine: String, appFile: String, roleArn: Int64, appDriverSize: String, appExecutorSize: String, appExecutorNums: Int64, sparkAppId: String, eni: String? = nil, isLocal: String? = nil, mainClass: String? = nil, appConf: String? = nil, isLocalJars: String? = nil, appJars: String? = nil, isLocalFiles: String? = nil, appFiles: String? = nil, isLocalPythonFiles: String? = nil, appPythonFiles: String? = nil, cmdArgs: String? = nil, maxRetries: Int64? = nil, dataSource: String? = nil, isLocalArchives: String? = nil, appArchives: String? = nil, sparkImage: String? = nil, sparkImageVersion: String? = nil, appExecutorMaxNumbers: Int64? = nil, sessionId: String? = nil, isInherit: UInt64? = nil, isSessionStarted: Bool? = nil, region: TCRegion? = nil, logger: Logger = TCClient.loggingDisabled, on eventLoop: EventLoop? = nil) -> EventLoopFuture<ModifySparkAppResponse> {
        self.modifySparkApp(.init(appName: appName, appType: appType, dataEngine: dataEngine, appFile: appFile, roleArn: roleArn, appDriverSize: appDriverSize, appExecutorSize: appExecutorSize, appExecutorNums: appExecutorNums, sparkAppId: sparkAppId, eni: eni, isLocal: isLocal, mainClass: mainClass, appConf: appConf, isLocalJars: isLocalJars, appJars: appJars, isLocalFiles: isLocalFiles, appFiles: appFiles, isLocalPythonFiles: isLocalPythonFiles, appPythonFiles: appPythonFiles, cmdArgs: cmdArgs, maxRetries: maxRetries, dataSource: dataSource, isLocalArchives: isLocalArchives, appArchives: appArchives, sparkImage: sparkImage, sparkImageVersion: sparkImageVersion, appExecutorMaxNumbers: appExecutorMaxNumbers, sessionId: sessionId, isInherit: isInherit, isSessionStarted: isSessionStarted), region: region, logger: logger, on: eventLoop)
    }

    /// 更新spark作业
    @inlinable @discardableResult
    public func modifySparkApp(appName: String, appType: Int64, dataEngine: String, appFile: String, roleArn: Int64, appDriverSize: String, appExecutorSize: String, appExecutorNums: Int64, sparkAppId: String, eni: String? = nil, isLocal: String? = nil, mainClass: String? = nil, appConf: String? = nil, isLocalJars: String? = nil, appJars: String? = nil, isLocalFiles: String? = nil, appFiles: String? = nil, isLocalPythonFiles: String? = nil, appPythonFiles: String? = nil, cmdArgs: String? = nil, maxRetries: Int64? = nil, dataSource: String? = nil, isLocalArchives: String? = nil, appArchives: String? = nil, sparkImage: String? = nil, sparkImageVersion: String? = nil, appExecutorMaxNumbers: Int64? = nil, sessionId: String? = nil, isInherit: UInt64? = nil, isSessionStarted: Bool? = nil, region: TCRegion? = nil, logger: Logger = TCClient.loggingDisabled, on eventLoop: EventLoop? = nil) async throws -> ModifySparkAppResponse {
        try await self.modifySparkApp(.init(appName: appName, appType: appType, dataEngine: dataEngine, appFile: appFile, roleArn: roleArn, appDriverSize: appDriverSize, appExecutorSize: appExecutorSize, appExecutorNums: appExecutorNums, sparkAppId: sparkAppId, eni: eni, isLocal: isLocal, mainClass: mainClass, appConf: appConf, isLocalJars: isLocalJars, appJars: appJars, isLocalFiles: isLocalFiles, appFiles: appFiles, isLocalPythonFiles: isLocalPythonFiles, appPythonFiles: appPythonFiles, cmdArgs: cmdArgs, maxRetries: maxRetries, dataSource: dataSource, isLocalArchives: isLocalArchives, appArchives: appArchives, sparkImage: sparkImage, sparkImageVersion: sparkImageVersion, appExecutorMaxNumbers: appExecutorMaxNumbers, sessionId: sessionId, isInherit: isInherit, isSessionStarted: isSessionStarted), region: region, logger: logger, on: eventLoop)
    }
}
